{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quiz2-Ala'a.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "PtaYGotfZ0E_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ea8ee23-afa8-4757-f95a-948cb6b392bd"
      },
      "cell_type": "code",
      "source": [
        "#The Code Question:\n",
        "#3. You have decided you want to create a neural network model:\n",
        "\n",
        "#a. Use train_test_split function to split the data into training 70%, test 30%\n",
        "\n",
        "#b. Design a neural network model with any architecture. Try to be unique\n",
        "#here.\n",
        "\n",
        "#c. Use Keras codes to create the model\n",
        "\n",
        "#d. Compile and fit the model\n",
        "\n",
        "#e. What is the accuracy after 20 epochs?\n",
        "\n",
        "# epochs 20 is :acc: 0.4717 means 47%\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# data\n",
        "\n",
        "\n",
        "\n",
        "df =  pd.read_csv('heart.csv')\n",
        "\n",
        "x = df.drop(['target'], axis=1).as_matrix()\n",
        "\n",
        "y = df.target.as_matrix()\n",
        "\n",
        "\n",
        "x_train,x_test, y_train, y_test= train_test_split(x,y,test_size=0.30, random_state =42)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sik8C6lcdyLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "d488317e-97f9-4aa6-b635-0db324c433e2"
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
        "# in the first layer, you must specify the expected input data shape:\n",
        "# here, 20-dimensional vectors.\n",
        "\n",
        "\n",
        "model.add(Dense(64, activation='relu', input_dim=13))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128)\n",
        "score = model.evaluate(x_test, y_test, batch_size=128)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "212/212 [==============================] - 0s 2ms/step - loss: 7.1702 - acc: 0.5330\n",
            "Epoch 2/20\n",
            "212/212 [==============================] - 0s 45us/step - loss: 7.6255 - acc: 0.4953\n",
            "Epoch 3/20\n",
            "212/212 [==============================] - 0s 44us/step - loss: 9.1721 - acc: 0.4245\n",
            "Epoch 4/20\n",
            "212/212 [==============================] - 0s 67us/step - loss: 8.7438 - acc: 0.4575\n",
            "Epoch 5/20\n",
            "212/212 [==============================] - 0s 45us/step - loss: 8.9716 - acc: 0.4434\n",
            "Epoch 6/20\n",
            "212/212 [==============================] - 0s 52us/step - loss: 8.9416 - acc: 0.4340\n",
            "Epoch 7/20\n",
            "212/212 [==============================] - 0s 44us/step - loss: 8.7436 - acc: 0.4575\n",
            "Epoch 8/20\n",
            "212/212 [==============================] - 0s 53us/step - loss: 7.8310 - acc: 0.5142\n",
            "Epoch 9/20\n",
            "212/212 [==============================] - 0s 32us/step - loss: 7.6789 - acc: 0.5236\n",
            "Epoch 10/20\n",
            "212/212 [==============================] - 0s 39us/step - loss: 8.4392 - acc: 0.4764\n",
            "Epoch 11/20\n",
            "212/212 [==============================] - 0s 39us/step - loss: 6.6905 - acc: 0.5849\n",
            "Epoch 12/20\n",
            "212/212 [==============================] - 0s 41us/step - loss: 7.7752 - acc: 0.5142\n",
            "Epoch 13/20\n",
            "212/212 [==============================] - 0s 41us/step - loss: 7.9830 - acc: 0.5047\n",
            "Epoch 14/20\n",
            "212/212 [==============================] - 0s 38us/step - loss: 7.5268 - acc: 0.5330\n",
            "Epoch 15/20\n",
            "212/212 [==============================] - 0s 36us/step - loss: 7.0855 - acc: 0.5566\n",
            "Epoch 16/20\n",
            "212/212 [==============================] - 0s 40us/step - loss: 8.2937 - acc: 0.4811\n",
            "Epoch 17/20\n",
            "212/212 [==============================] - 0s 40us/step - loss: 7.7552 - acc: 0.5189\n",
            "Epoch 18/20\n",
            "212/212 [==============================] - 0s 41us/step - loss: 8.5152 - acc: 0.4717\n",
            "Epoch 19/20\n",
            "212/212 [==============================] - 0s 39us/step - loss: 7.6789 - acc: 0.5236\n",
            "Epoch 20/20\n",
            "212/212 [==============================] - 0s 42us/step - loss: 8.5153 - acc: 0.4717\n",
            "91/91 [==============================] - 0s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VEGE7Yp_eeDp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#The Theory part:\n",
        "\n",
        "#1. Using the problem and data at https://www.kaggle.com/ronitf/heart-disease-uci,\n",
        "download the data (3KB)\n",
        "\n",
        "#2. What type of problem are we trying to solve here? Classification or regression?\n",
        "Since the goal is to identify refers to the presence of heart disease in the patient, so it is a classification problem.\n",
        "\n",
        "#3.e. What is the accuracy after 20 epochs?\n",
        "\n",
        "epochs 20 is :acc: 0.4717 means 47%\n",
        "\n",
        "#4. What is underfitting and overfitting?\n",
        "\n",
        "Overfitting:\n",
        "  Occurs when a statistical model or machine learning algorithm captures the noise of the data.  Intuitively, overfitting occurs when the model or the algorithm fits the data too well.  Specifically, overfitting occurs if the model or algorithm shows low bias but high variance.  Overfitting is often a result of an excessively complicated model, and it can be prevented by fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data.\n",
        "\n",
        "\n",
        "Underfitting :occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data.  Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough.  Specifically, underfitting occurs if the model or algorithm shows low variance but high bias.  Underfitting is often a result of an excessively simple model.\n",
        "\n",
        "  #Both overfitting and underfitting lead to poor predictions on new data sets.\n",
        "\n",
        "#5. Explain how you can reduce the overfitting of the model.\n",
        "\n",
        "Overfitting is often a result of an excessively complicated model, and it can be prevented by fitting multiple models and using validation or cross-validation to compare their predictive accuracies on test data. It can be also done by:\n",
        "1. Reduce the networkâ€™s capacity by removing layers or reducing the number of elements in the hidden layers\n",
        "2.Apply regularization, which comes down to adding a cost to the loss function for large weights\n",
        "3.Use Dropout layers, which will randomly remove certain features by setting them to zero\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}